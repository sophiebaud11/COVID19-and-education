---
title: "clean_data.Rmd"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(totalcensus)
library(maps)
library(gganimate)
library(lubridate)
library(rstanarm)
library(tidymodels)
library(gtsummary)
library(broom.mixed)
library(gt)
library(rsample)
library(nnet)
```
 
 
# Load Data

```{r}

# read in school covid restriction data from randomly selected districts; data 
# was collected by manually searching the websites of the sampled districts
# based on a code of 0 for not entirely virtual and 1 for entirely virtual.

covid_restrictions <- read.csv("raw_data/covid_restrictions.csv") %>%
  
  # get rid of the few counties with unsuccessful data collection
  
  filter(covid.restriction == -1 | covid.restriction == 0 | 
           covid.restriction == 1) %>%
  
  # make date column into the date type
  
  mutate(date = mdy(date))

# read in the list of sampled districts, calculated using the sampele() 
# function  and then verified (in terms of county location) manually. 

samples <- read.csv("raw_data/sample_districts_edited.csv")

# read in the covid-19 case count data from the NY Times, and filter for only
# those counties contained in the sample as well as dates after July 31st
# (to approximate the start of the school year).

sample_counties_covid_nyt <- read.csv("raw_data/us-counties.csv") %>%
  filter(fips %in% samples$fips_code) %>%
  mutate(date = as_date(date)) %>%
  filter(date > ymd(20200731))
  

# load in geographic school district data to sample, sourced from:
# Steven Manson, Jonathan Schroeder,David Van Riper, Tracy Kugler, and 
# Steven Ruggles. IPUMS National Historical Geographic Information System: 
# Version 15.0 [dataset]. Minneapolis, MN: IPUMS. 2020. 
# http://doi.org/10.18128/D050.V15.0 / IPUMS NHGIS, University of Minnesota, 
# www.nhgis.org (2014-2018).

district_uni  <- read.csv("raw_data/districts_uni.csv") %>%
  select(c(GISJOIN, STATE, STATEA, SDUNI, SDUNIA, AJWME001, AJWMM001))

# read in average annual teacher salary by state (2018-2019) data source: 
# https://nces.ed.gov/programs/digest/d19/tables/dt19_211.60.asp

avg_salary_2018 <- read.csv("raw_data/avg_teacher_salary.csv")


# read in county data from: Quarterly Census of Employment and Wages
# (https://data.bls.gov/cew/apps/table_maker/v4/table_maker.
# htm#type=1&year=2020&qtr=1&own=3&ind=6111&supp=0)

county_data <- read.csv("raw_data/county_wages.csv")


state_names <- tibble(state.abb, state.name) %>%
  rename(state = "state.abb")

# read in fips codes sourced from:
# https://www.nrcs.usda.gov/wps/portal/nrcs/detail/national/home/?
# cid=nrcs143_013697

fips_codes <- read.csv("raw_data/fips_codes.csv") %>%
  left_join(., state_names, by = "state")

# read in covid-19 case count by county sourced from: 
# https://github.com/nytimes/covid-19-data. 

county_covid_nyt <- read.csv("raw_data/us-counties.csv")

# read in 2019 county populations (estimated) from:
# https://www.census.gov/data/datasets/time-series/demo/popest/2010s-counties-
# total.html#par_textimage_70769902.

county_pop <- read.csv("raw_data/census_county_population.csv")

```


# Generate Sample

```{r}

  
# sample 50 random counties and get the school districts for those counties
# rep = 2 to account for cases in which the school district doesn't have info
# online. (note: any samples generated by this function after the creation of 
# the model will not correspond to the samples used in the model; those are 
# located in the sampled_districts_edited csv file. this code is included
# simply to indicate how I underwent these calculations!)

sample_1 <- sample(district_uni, size = 50, replace = FALSE)

# replace initial size 50 sample with an additional sample of 7 to replace the
# 7 districts that didn't fit neatly into counties

sample_districts_GIS_extra <- sample(district_uni$GISJOIN, size = 7, 
                                     replace = FALSE)

# get district data from those new sampled districts

sample_districts_extra <- district_uni %>%
  filter(GISJOIN %in% sample_districts_GIS_extra) 

# combine first and second samples

sample_districts <- sample_1 %>%
  filter(GISJOIN %in% sample_districts_GIS) %>%
  rbind(., sample_districts_extra)

# remove the wonky districts

sample_districts <- sample_districts[-c(11, 18, 20, 22, 23, 37, 38), ]

# add county fips codes so you can graph it

sample_districts <- sample_districts %>%
  mutate(fips_code = c(1043, 8081, 9013, 13037, 16027, 
                         17043, 19193, 19121, 20019, 23005,
                         25015, 26021, 28087, 28047, 28149,
                         29033, 29183, 31055, 34003, 34027,
                         34009, 36103, 36077, 36079, 36059,
                         38031, 39031, 39051, 40079, 40113,
                         40019, 42037, 42075, 42085, 46033,
                         48439, 48347, 48029, 53077, 53065,
                         53005, 55045, 55029, 13269, 18039,
                         26093, 26017, 31097, 39145, 48097)) %>%
  left_join(., fips_codes, by = "fips_code") 

# rename columns so it can be easily graphed later on

sample_districts_edited <- sample_districts %>%
  mutate(county = tolower(county)) %>%
  rename(subregion = "county") %>%
  mutate(state_county = tolower(paste(STATE, subregion, sep = "")))

```


# Wrangle Sample Data for the Model and for Graphing

```{r}

# clean county wage data so that it only displays weekly wages for public 
# schools.

county_data_edited <- county_data %>%
  slice(1:5444) %>%
  filter(own_code != 5) %>%
  select(c(area_fips, avg_wkly_wage, own_code)) %>%
  rename(fips_code = "area_fips") %>%
  mutate(fips_code = as.numeric(fips_code),
         avg_wkly_wage = as.numeric(avg_wkly_wage)) %>%
  
  # join this data with the fips_codes object, which contains county and 
  # state names for each fips code
  
  left_join(., fips_codes, by = "fips_code") %>%
  mutate(avg_wkly_wage = na_if(avg_wkly_wage, 0),
         county = tolower(county)) %>%
  drop_na() %>%
  rename(subregion = "county") %>%
  
  # create a state_county for ease of graphing later on.
  
  mutate(state_county = tolower(paste(state.name, subregion, sep = "")))

# store the above object in a new object to use in grahin, that is stored
# and averaged by state_county.

county_wage_data <- county_data_edited %>%
  group_by(state_county) %>%
  summarize(avg_wkly_wage_all = mean(avg_wkly_wage), .groups = "drop")
  

# clean county population data 

county_pop_edited <- county_pop %>%
  
  # filter to only include counties
  
  filter(str_detect(CTYNAME, "County") | str_detect(CTYNAME, "Parish")) %>%
  rename(state = "STNAME") %>%
  
  # create subregion and state_county columsn for graphing later on that 
  # accounts for the  fact that many counties are called "Parishes" not 
  # "Counties."
  
  mutate(subregion = if_else(str_detect(CTYNAME, "County"),  
                             tolower(trimws(gsub("County", "", CTYNAME))), 
         tolower(trimws(gsub("Parish", "", CTYNAME)))))%>%
  mutate(state_county = tolower(paste(state, subregion, sep = "")))

# create a new object with a state_county column for each fips code

county_fips <- fips_codes %>%
  mutate(state_county = tolower(paste(state.name, county, sep = "")))

# create a new object with the wrangled fips code data & population data.

county_pop_with_fips <- merge(county_fips, county_pop_edited, 
                              by = "state_county") %>%
  select(fips_code, POPESTIMATE2019, state_county) 

# combine the COVID-19 restrictions data I manually collected with the daily
# COVID-19 case count per county data from the NY Times by  date and county.

final_sample_data <- merge(covid_restrictions, sample_counties_covid_nyt, 
                               by = c("date", "county")) %>%
  
  # get rid of unneeded columns & rename columns for convenience & clarity
  
  select(-c(fips_code, state.y, )) %>%
  rename(state = "state.x",
         fips_code = "fips") %>%
  
  # combine with the obbject that contains population data for each county,
  # organized by fips code.
  
  merge(., county_pop_with_fips, by = "fips_code") %>%
  
  # create a cases per capita colummn & make the covid.restriction column
  # numeric.
  
  mutate(cases_per_capita = cases / POPESTIMATE2019,
         covid.restriction = as.numeric(covid.restriction))

# edit the above object so that the codes are changed to just 0 for not 
# entirely virtual and 1 for entirely virtual. store this in a new object!

final_sample_data_binomial <- final_sample_data %>%
  mutate(covid.restriction = ifelse(covid.restriction == -1, 
                                    0, covid.restriction)) %>%
  
  # calculate daily rate of change for every day of every county.
  
  group_by(county) %>% 
  arrange(county, date) %>% 
  mutate(rate = 100 * 
           (cases_per_capita - lag(cases_per_capita))/lag(cases_per_capita)) %>%
  ungroup()

```


# Generate Model

```{r}

# build the model!

# load in state data in order to create the regions tibble

data(state)

# store the collection of regions by state name in the regions object

regions <- tibble(state.name, state.region) %>%
  rename(state = "state.name") %>%
  mutate(state.region = ifelse(state.region == "North Central", "Midwest", as.character(state.region)))

# finalize data to be used in the model by combining the sample data with the
# regions tibble, and edit the names

final_fit_data <- left_join(final_sample_data_binomial, regions, 
                            by = "state") %>%
  rename(`COVID Rate` = "rate",
         Region = "state.region")

# create an object to contain the summarized data for each sample to store in
# the samples tab of my Shiny app.

final_fit_table <- final_fit_data %>%
  select(school.district, covid.restriction, `COVID Rate`, 
         Region, county, state) %>%
  group_by(county) %>%
  summarize(`Mean COVID Restriction Level`= mean(covid.restriction, 
                                                 na.rm = TRUE), 
            `Mean COVID Daily Rate of Change (Per Capita)` = 
              mean(`COVID Rate`, na.rm = TRUE),
            `School District` = first(school.district), County = first(county), 
            State = first(state), Region = first(Region), .groups = "drop") %>%
  select(-county)

# save this data in an RDS file for easy access in the Shiny app.

saveRDS(final_fit_table, "rds_data/district_samples_table.RDS")

set.seed(9)

# create a stan_glm binomial model which measures the impact of COVID 
# daily rate of change and region on school reopening restrictions. I saved
# this model as the fit.RDS file!

fit2 <- stan_glm(formula = covid.restriction ~ `COVID Rate` + Region - 1,
                 data = final_fit_data,
                 family = binomial(),
                 refresh = 0)



```


# Survey Graph Data

```{r}

# make graphs based on ipsos survey data

# read in the survey data to use as a source for each graph

ipsos_survey <- readxl::read_xlsx("raw_data/ipsos_survey.xlsx")

# create an object with the survey data needed for the pie chart. I saved
# that data as an RDS file for easy access in the Shiny app!

pie_data <- tibble(answer = c("Remote, virtual learning.", 
                              "In-person learning."), values = c(0.66,0.34))

# do the above for each of the other topics:

bar_chart_remote_data <- tibble(Question = c(
  "I'm concerned about distance learning 
  being an effective way for students to learn.", 
  "I'm concerned that distance learning 
  will create gaps in opportunities for my students."), Agree = c(0.81, 0.84))

bar_chart_inperson_data <- tibble(Question = c(
  "I am concerned about risking my 
  own health by returning to school.",
  "I am concerned about 
  accessing sufficient PPE 
  and cleaning materials 
  for in-person teaching.",
  "I would struggle to enforce social
  distancing among my students."), 
  Agree = c(0.77, 0.78, 0.84))

communication_chart_data <- tibble(
  Question = c(rep("I have a voice in how my \nschool district responds 
                   \nto the COVID-19 pandemic.", 5),
               rep("My school district has provided \nsufficient training or 
                   preparation \nfor the upcoming school year", 5)), 
  Response = c(rep(c("Strongly agree", "Somewhat agree", 
               "Somewhat disagree", "Strongly disagree", 
               "Don't know"), 2)),
  Value = c(0.08, 0.32, 0.32, 0.27, 0.01, 
            0.09, 0.28, 0.35, 0.26, 0.02)) %>%
  mutate(ResponseValue = paste(Response, (Value * 100), sep = ": "))

```


# Generate Map Data

```{r}

# load county_map data so I can map my tibbles.

county_map <- map_data("county") %>%
  mutate(state_county = tolower(paste(region, subregion, sep = ""))) 

# join the COVID-19 data and county_map data by region, so that the COVID-19 
# data for each state can be mapped to all of the necessary latitutes / 
# longitudes. also, deselect unnecessary columns!

county_covid_map <- left_join(county_map, county_covid_edited, 
                              by = "state_county")

# do the same for the county wage data

county_wage_map <- left_join(county_map, county_wage_data, 
                             by = c("state_county"))

```


